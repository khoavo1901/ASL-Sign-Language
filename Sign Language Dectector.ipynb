{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36f150b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch, torchvision\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f37fb272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/davidbau/baukit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63f1c5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baukit import show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a20e96",
   "metadata": {},
   "source": [
    "## Cuting the frame into 48x48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "147dba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setting up the directory to transfer the data to\n",
    "# directory = 'SignImage48x48'\n",
    "# if not os.path.exists(directory):\n",
    "#     os.mkdir(directory)\n",
    "# if not os.path.exists(f'{directory}/blank'):\n",
    "#     os.mkdir(f'{directory}/blank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87bd721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # range 65 to 91 is just the alphabet from A to Z when transform from into character\n",
    "# for i in range(65, 91):\n",
    "#     letter = chr(i)\n",
    "#     if not os.path.exists(f'{directory}/{letter}'):\n",
    "#         os.mkdir(f'{directory}/{letter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09d25fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(0)\n",
    "# while True:\n",
    "#     _, frame = cap.read()\n",
    "#     count = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c6bfed",
   "metadata": {},
   "source": [
    "## ASL CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95c2eea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASL_Model(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ASL_Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.conv2 = nn.Conv2d(128, 256, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(256, 512, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(512, 512, kernel_size=3)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128, 512)  # Adjust the size accordingly\n",
    "        self.dropout1 = nn.Dropout(p=0.4)\n",
    "        self.fc2 = nn.Linear(512, 64)\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        self.fc3 = nn.Linear(64, 256)\n",
    "        self.dropout3 = nn.Dropout(p=0.3)\n",
    "        self.fc4 = nn.Linear(256, 64)\n",
    "        self.dropout4 = nn.Dropout(p=0.2)\n",
    "        self.fc5 = nn.Linear(64, 256)\n",
    "        self.dropout5 = nn.Dropout(p=0.3)\n",
    "        self.fc6 = nn.Linear(256, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.dropout5(x)\n",
    "        \n",
    "        x = self.fc6(x)\n",
    "        return F.softmax(x, dim=num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92764aa5",
   "metadata": {},
   "source": [
    "## Loading the ASL Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd100ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfdc699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"archive/asl_alphabet_train/asl_alphabet_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eeb8b23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the training set = 87000\n"
     ]
    }
   ],
   "source": [
    "train_set = ImageFolder(train_path, transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((48, 48)),\n",
    "transforms.ToTensor()]))\n",
    "\n",
    "print(\"Number of images in the training set =\", len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dec8311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14400th item is a pair (tensor([[[0.1412, 0.1569, 0.1647,  ..., 0.4549, 0.4627, 0.3922],\n",
      "         [0.1647, 0.1922, 0.1961,  ..., 0.6706, 0.6824, 0.5804],\n",
      "         [0.2078, 0.2667, 0.2588,  ..., 0.6118, 0.6235, 0.5490],\n",
      "         ...,\n",
      "         [0.3529, 0.5176, 0.5216,  ..., 0.5412, 0.5373, 0.4588],\n",
      "         [0.3569, 0.5216, 0.5216,  ..., 0.5412, 0.5333, 0.4588],\n",
      "         [0.3137, 0.4471, 0.4510,  ..., 0.4667, 0.4588, 0.3961]]]), 4)\n"
     ]
    }
   ],
   "source": [
    "idx = 14400\n",
    "item = train_set[idx]\n",
    "print(f\"{idx}th item is a pair\", item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b02643c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size = 128,\n",
    "    shuffle = True,\n",
    "    num_workers=2,\n",
    "    pin_memory = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72a796e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    # initiate a loss monitor\n",
    "    train_loss = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        # predict the class\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        predicted = model(images)\n",
    "        loss = loss_fn(predicted, labels)\n",
    "        correct_predictions += (predicted.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "    \n",
    "    return np.mean(train_loss), correct_predictions / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcd3e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 1, out_channels = 128, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Conv2d(128, 256, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            nn.Conv2d(256, 512, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            nn.Conv2d(512, 512, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            nn.AdaptiveAvgPool2d(output_size = (1,1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features = 512, out_features = 512),  # Adjust the size accordingly\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(256, len(train_set.classes))\n",
    "        ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ef485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ASL_Model(len(train_set.classes)).to(device)\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.001\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "training_losses = []\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss, train_acc = train_model(model, train_loader, loss_fn, optimizer)\n",
    "    training_losses.append(train_loss)\n",
    "    print(f\"epoch {epoch+1}/{epochs} | train loss={np.mean(train_loss):.4f}, {train_acc=:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6efcb17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4546c34f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
